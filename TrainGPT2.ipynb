{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TrainGPT2.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"zReQZ6kJ90H-"},"source":["# Settings"]},{"cell_type":"markdown","metadata":{"id":"vbXliRYT-u1F"},"source":["Mount Google Drive to `/content/drive`."]},{"cell_type":"code","metadata":{"id":"A8pgsgy-ICwh"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Zszi8xz4wUE"},"source":["\n","   Modify the \"DRIVE_PATH\" variable with the location of the 'models', 'checkpoint', and 'samples' folders."]},{"cell_type":"code","metadata":{"id":"vXHY_ZK0-qeL"},"source":["# Use me to explorer your Google Drive.\n","\n","!ls -lha \"/content/drive/MyDrive/RPGMaker2005 (Group 4)/training\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DXAFlDvM4yIq"},"source":["import os\n","\n","# Change me to the location of 'models', 'checkpoint', and 'samples'.\n","DRIVE_PATH = \"/content/drive/MyDrive/RPGMaker2005 (Group 4)/training\"\n","\n","# Change me to your desired dataset text file.\n","TRAINING_DATA = os.path.join(DRIVE_PATH, \"new-dataset.txt\")\n","\n","ENV_SETTINGS = {}\n","ENV_SETTINGS[\"DRIVE_PATH\"] = DRIVE_PATH\n","ENV_SETTINGS[\"MODELS_PATH\"] = os.path.join(DRIVE_PATH, \"models\")\n","ENV_SETTINGS[\"CHECKPOINT_PATH\"] = os.path.join(DRIVE_PATH, \"checkpoint\")\n","ENV_SETTINGS[\"SAMPLES_PATH\"] = os.path.join(DRIVE_PATH, \"samples\")\n","ENV_SETTINGS[\"TRAINING_DATA\"] = TRAINING_DATA\n","\n","def setEnviron(envDict: dict):\n","  for env in envDict:\n","    os.environ[env] = envDict[env]\n","    print(\"Set environment variable: \" + env + \"=\" + envDict[env])\n","\n","setEnviron(ENV_SETTINGS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3tzLEBp84l2O"},"source":["# GPT-2 Fine-tuning via Google Colab\n","\n","All the required code to start training the AI model should be here and in-order.\n","\n","You may need to change a few things to match your setup."]},{"cell_type":"markdown","metadata":{"id":"HE02SjM27959"},"source":["1. Clone gitlab repository and change directory. (please don't share my api key)"]},{"cell_type":"code","metadata":{"id":"b-vJgvzo49dO"},"source":["!git clone --recursive \"https://oauth2:nnsTeRq-kac_WezGE2zo@charon.cs.uni.edu/jmcclain/rpg-maker-2005.git\"\n","%cd rpg-maker-2005"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C8al8amDGkNz"},"source":["2. Install requirements via pip"]},{"cell_type":"code","metadata":{"id":"Rp3_ht_P8IYx"},"source":["# !pip install -r requirements.txt # We only need stuff from gpt-2\n","!ls\n","!pip install -r gpt_2/requirements.txt\n","!pip install tensorflow==2.4.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uAPW4a-xG928"},"source":["3. Change directory to gpt-2 and apply fix for <|endoftext|>"]},{"cell_type":"code","metadata":{"id":"Q1C8fPU2HGIM"},"source":["%cd gpt_2\n","# Download patch for '<|endoftext|>'\n","!wget https://github.com/nshepperd/gpt-2/commit/26e6d2b3c9f9aab743f283defcadbe025c3bb9c4.patch -O endoftext.patch\n","# Apply patch\n","!patch -Np1 -i endoftext.patch\n","# Show changes\n","!git diff"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZgCtWYoJLSSU"},"source":["4. Link 'checkpoint', 'samples' and 'models' folders (Double check your Settings above)"]},{"cell_type":"code","metadata":{"id":"gVc6blOGLYvz"},"source":["import os\n","\n","# Get values from environment variables.\n","modelsPath = os.environ[\"MODELS_PATH\"]\n","checkpointPath = os.environ[\"CHECKPOINT_PATH\"]\n","samplesPath = os.environ[\"SAMPLES_PATH\"]\n","\n","# Simpily creates a symlink of a folder/file in the current directory.\n","def createSymLink(path):\n","  os.symlink(path, os.path.basename(path))\n","\n","# create symlinks in the current directory\n","createSymLink(modelsPath)\n","createSymLink(checkpointPath)\n","createSymLink(samplesPath)\n","\n","# List the contents to confirm things are linked correctly.\n","!ls -lha"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2g4w4guPMHlm"},"source":["6. Download model (If you've ran this before than you can skip it.)"]},{"cell_type":"code","metadata":{"id":"mjtxrOGcMJQi"},"source":["!python download_model.py 355M"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WFPNhZxxJkdH"},"source":["7. Create 'encoded.npz'"]},{"cell_type":"code","metadata":{"id":"c8aa5ibKJqqJ"},"source":["!echo \"$TRAINING_DATA\"\n","!PYTHONPATH=src python encode.py --model_name 355M \"$TRAINING_DATA\" encoded.npz\n","!ls -lha"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qmXM9ZFl1HmN"},"source":["  pre8. Setup Tensor Rematerialization (Better Memory)\n","\n","  The thing that `train.py` interfaces with for reduced memory usage is written in Haskell, so we have to jump through some hoops to get the Haskell compiler and package manager installed before compiling the program."]},{"cell_type":"code","metadata":{"id":"n7h2lJot1HAr"},"source":["import os\n","import shutil\n","\n","CACHED_CABAL = os.path.join(os.environ['DRIVE_PATH'], 'cabal-cache.zip')\n","\n","# Restore '/root/.cabal' from cache\n","if os.path.isfile(CACHED_CABAL):\n","  shutil.unpack_archive(CACHED_CABAL, '/root/.cabal')\n","\n","# Install a working ghc/cabal-install version (via haskell ppa)\n","!add-apt-repository ppa:hvr/ghc --yes\n","!apt-get update\n","!apt-get install ghc-9.0.1 cabal-install-3.4\n","\n","# Compile twremat\n","%cd twremat\n","# For some reason we have too specify the PATH\n","!PATH=\"/opt/cabal/bin:/opt/ghc/bin:${PATH}\" cabal update\n","!PATH=\"/opt/cabal/bin:/opt/ghc/bin:${PATH}\" cabal v2-install --installdir=../bin\n","# make sure '../bin/twremat' is executable.\n","!chmod +x \"../bin/twremat\"\n","%cd ..\n","\n","# Save '/root/.cabal'\n","shutil.make_archive(CACHED_CABAL.split('.')[0], 'zip', '/root/.cabal', verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qsiR7UouMxB7"},"source":["8. Start training"]},{"cell_type":"code","metadata":{"id":"TTeQy9dKRiLD"},"source":["# Use this to show gpu info via wandb\n","!pip install wandb\n","import wandb\n","wandb.init()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MJ_iriKEKbVK"},"source":["!PYTHONPATH=src python train.py \\\n","  --sample_every 250 \\\n","  --dataset encoded.npz \\\n","  --model_name 355M \\\n","  --optimizer sgd \\\n","  --learning_rate 0.0006"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XfaW-mpY0nvG"},"source":["9. Update model zip\n","\n","Grabs the required GPT2 model files and creates a zip archive."]},{"cell_type":"code","metadata":{"id":"-_yFLrBe0sjr"},"source":["import os\n","import shutil\n","import json\n","import hashlib\n","\n","CHECKPOINT_RUN_PATH = os.path.join(os.environ['CHECKPOINT_PATH'], 'run1')\n","MODEL_PATH = os.path.join(os.environ['MODELS_PATH'], '355M')\n","\n","OUTPUT_DIR = os.path.join(os.environ['DRIVE_PATH'], \"rpg-model\")\n","OUTPUT_HASH = os.path.join(os.environ['DRIVE_PATH'], \"rpg-model_blake2b.json\")\n","OUTPUT_ZIP = os.path.join(os.environ['DRIVE_PATH'], \"rpg-model\")\n","CHECKPOINT_FILE_LIST = ['checkpoint', 'counter']\n","MODEL_FILE_LIST = ['encoder.json', 'hparams.json', 'vocab.bpe']\n","\n","print(\"Will output to:\", OUTPUT_DIR)\n","\n","# Make a clean directory\n","print(\"Cleaning\", OUTPUT_DIR)\n","if os.path.isdir(OUTPUT_DIR):\n","  for file in os.listdir(OUTPUT_DIR):\n","    file = os.path.join(OUTPUT_DIR, file)\n","    os.remove(file)\n","  os.rmdir(OUTPUT_DIR)\n","\n","print(\"Creating\", OUTPUT_DIR)\n","os.mkdir(OUTPUT_DIR)\n","\n","# Get current checkpoint\n","print(\"Getting latest checkpoint\")\n","checkpoint_num = \"\"\n","with open(os.path.join(CHECKPOINT_RUN_PATH, \"counter\"), 'r') as f:\n","  for line in f.readlines():\n","    checkpoint_num += line\n","  checkpoint_num = int(checkpoint_num)\n","\n","# Copy checkpoint files\n","print(\"Copying checkpoint files\")\n","for file in os.listdir(CHECKPOINT_RUN_PATH):\n","  if (str(checkpoint_num) in file) and (\"tfevents\" not in file):\n","    CHECKPOINT_FILE_LIST.append(file)\n","\n","for file in CHECKPOINT_FILE_LIST:\n","  file_path = os.path.join(CHECKPOINT_RUN_PATH, file)\n","  shutil.copyfile(file_path, os.path.join(OUTPUT_DIR, file))\n","\n","# Copy model files\n","print(\"Copying model files\")\n","for file in MODEL_FILE_LIST:\n","  file_path = os.path.join(MODEL_PATH, file)\n","  shutil.copyfile(file_path, os.path.join(OUTPUT_DIR, file))\n","\n","# Create a blake2b hash for each file and store them\n","# in a json file.\n","blake2b_hashes = {}\n","for file in os.listdir(OUTPUT_DIR):\n","  file_path = os.path.join(OUTPUT_DIR, file)\n","  blake2b_hash = hashlib.blake2b()\n","  with open(file_path, \"rb\") as f:\n","    chunk = f.read(8192)\n","    while chunk:\n","      blake2b_hash.update(chunk)\n","      chunk = f.read(8192)\n","    blake2b_hashes[file] = blake2b_hash.hexdigest()\n","with open(OUTPUT_HASH, \"w\") as f:\n","  json.dump(blake2b_hashes, f, indent=2)\n","\n","# Zip up the OUTPUT_DIR\n","print(\"Zipping:\", OUTPUT_DIR, \"to\", OUTPUT_ZIP + \".zip\")\n","shutil.make_archive(OUTPUT_ZIP, \"zip\", OUTPUT_DIR)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sdyt1Pb1EMfg"},"source":["# Utils"]},{"cell_type":"markdown","metadata":{"id":"R1In0QoXEYLM"},"source":["1. Display information about Google Colab's Nvidia GPU\n","\n","   (Will fail if GPU is not selected in `Edit->Notebook Settings`)"]},{"cell_type":"code","metadata":{"id":"jbXgdcGjEQIY"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0bkZy2Isib9y"},"source":["2. Read samples"]},{"cell_type":"code","metadata":{"id":"f3PD2FNXibDT"},"source":["import os\n","\n","samplesPath = os.environ[\"SAMPLES_PATH\"]\n","\n","samples = [os.path.join(samplesPath, \"run1\", x) for x in os.listdir(os.path.join(samplesPath, \"run1\"))][-10:]\n","\n","def printFile(filename: str):\n","    with open(filename, \"r\") as f:\n","      return f.read()\n","\n","for sample in samples:\n","  print(printFile(sample))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pX8qo_-dpDH"},"source":["3. Clean checkpoints"]},{"cell_type":"code","metadata":{"id":"2xUQhE2BdtlN"},"source":["import os\n","\n","CHECKPOINT_RUN_PATH = os.path.join(os.environ['CHECKPOINT_PATH'], 'run1')\n","DONT_DELETE = ('checkpoint', 'counter', 'tfevents')\n","\n","# Get latest epoch\n","checkpoint = \"\"\n","with open(os.path.join(CHECKPOINT_RUN_PATH, 'counter'), 'r') as f:\n","  for line in f.readlines():\n","    checkpoint += line\n","  checkpoint = checkpoint.splitlines()[0]\n","\n","# Get list of file in CHECKPOINT_RUN_PATH\n","delete_me = os.listdir(CHECKPOINT_RUN_PATH)\n","\n","# Create a list of files to not delete\n","filter = []\n","for dont in DONT_DELETE:\n","  for file in delete_me:\n","    if (dont in file) or (checkpoint in file):\n","      filter.append(file)\n","\n","# Filter the delete_me list with the filter list.\n","delete_me = [x for x in delete_me if x not in filter]\n","\n","# Delete files in delete_me\n","for file in delete_me:\n","  file = os.path.join(CHECKPOINT_RUN_PATH, file)\n","  if os.path.isfile(file):\n","    os.remove(file)\n","\n","# List Files\n","newlist = os.listdir(CHECKPOINT_RUN_PATH)\n","newlist.sort()\n","for x in newlist:\n","  print(x)"],"execution_count":null,"outputs":[]}]}